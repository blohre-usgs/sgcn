{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In working through the SGCN problem with some new knowledge and thinking about interacting with ScienceBase, I'm experimenting here with a way to gather up every species name into one overall table to operate against. I still need to do a little thinking about how to deal with new data coming in over time in terms of how those should be stored in our final index. The process here is pretty straightforward and relies heavily on some Pandas fu. It uses sciencebasepy to grab every item from the source collection and then works them over to retrieve the flagged data files and take some initial cleanup steps.\n",
    "\n",
    "This produces one overall dataset out of every state list surprisingly quickly. I think the idea will be to run this process periodically, using the file date from the ScienceBase-stored files to see if there are any new state lists that need to be updated. I just haven't figured out what all to store from older data and how to run the API to pull the most current information.\n",
    "\n",
    "At the end, I dump this dataset to a CSV for later use. I experimented with the Pandas groupby capability which is really cool! Once we set up out clean_scientific_name field here, we can pull the CSV back into a dataframe, groupby that field, and have our unique names to lookup. After running the taxonomic authority consultation against ITIS and WoRMS, we can create an updated dataset with at least the ITIS TSN identifier for cases where it's best to go after related information with the ID vs. the name.\n",
    "\n",
    "This new process should work in a much more efficient manner compared to the old. We should only be running and storing individual records from each of the sources we consult with (taxonomic authorities, related data systems) tied by name or ID to the SGCN list. We should really be able to cache all of this related information in a data store somewhere that is leveraged and continually updating based on any incoming vector. Species lists can consult with the cache to see what's there, call for an update (earlier than some type of logical schedule) if they want, and pull back whatever they want to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sciencebasepy import SbSession\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import bispy\n",
    "\n",
    "bis_utils = bispy.bis.Utils()\n",
    "itis = bispy.itis.Itis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb = SbSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgcn_base_item = sb.get_item('56d720ece4b015c306f442d5')\n",
    "\n",
    "historic_national_list_file = next((f[\"url\"] for f in sgcn_base_item[\"files\"] if f[\"title\"] == \"Historic 2005 SWAP National List\"), None)\n",
    "if historic_national_list_file is not None:\n",
    "    historic_national_list = requests.get(historic_national_list_file).text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_historic_list(scientificname):\n",
    "    if scientificname in historic_national_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"parentId\": \"56d720ece4b015c306f442d5\",\n",
    "    \"fields\": \"title,dates,files,tags\",\n",
    "    \"max\": 500\n",
    "}\n",
    "\n",
    "items = sb.find_items(params)\n",
    "\n",
    "sgcn_items = list()\n",
    "while items and 'items' in items:\n",
    "    sgcn_items.extend(items[\"items\"])\n",
    "    items = sb.next(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 1.08 s, total: 12.1 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "source_data = list()\n",
    "for index, item in enumerate(sgcn_items):\n",
    "    data_file = next(l[\"url\"] for l in item[\"files\"] if l[\"title\"] == \"Process File\")\n",
    "    \n",
    "    try:\n",
    "        df_src = pd.read_csv(data_file, delimiter=\"\\t\")\n",
    "    except UnicodeDecodeError:\n",
    "        df_src = pd.read_csv(data_file, delimiter=\"\\t\", encoding='latin1')\n",
    "    \n",
    "    # Make lower case columns to deal with slight variation in source files\n",
    "    df_src.columns = map(str.lower, df_src.columns)\n",
    "\n",
    "    # Set the file updated date from the ScienceBase file to each record in the dataset for future reference\n",
    "    df_src[\"source_file_date\"] = next(l[\"dateUploaded\"] for l in item[\"files\"] if l[\"title\"] == \"Process File\")\n",
    "    \n",
    "    # Set the state name from the ScienceBase Item tag if needed\n",
    "    if \"state\" not in df_src.columns:\n",
    "        df_src[\"state\"] = next(t[\"name\"] for t in item[\"tags\"] if t[\"type\"] == \"Place\")\n",
    "\n",
    "    # Set the reporting year from the ScienceBase Item date if needed\n",
    "    if \"year\" not in df_src.columns:\n",
    "        df_src[\"year\"] = next(d[\"dateString\"] for d in item[\"dates\"] if d[\"type\"] == \"Collected\")\n",
    "    \n",
    "    # Get rid of the reported '2005 SWAP' column because we can't count on it and it's too messy\n",
    "    if \"2005 swap\" in df_src.columns:\n",
    "        df_src.drop(\"2005 swap\", axis=1, inplace=True)\n",
    "        \n",
    "    # Standardize naming of the reported taxonomic group column (though we may get rid of this eventually)\n",
    "    if \"taxonomy group\" in df_src.columns:\n",
    "        df_src.rename(columns={\"taxonomy group\": \"taxonomic category\"}, inplace=True)\n",
    "\n",
    "    # Take care of the one weird corner case\n",
    "    if \"taxonomy group (use drop down box)\" in df_src.columns:\n",
    "        df_src.rename(columns={\"taxonomy group (use drop down box)\": \"taxonomic category\"}, inplace=True)\n",
    "\n",
    "    # Clean up the scientific name string for lookup by applying the function from bis_utils\n",
    "    df_src[\"clean_scientific_name\"] = df_src.apply(lambda x: bis_utils.clean_scientific_name(x[\"scientific name\"]), axis=1)\n",
    "\n",
    "    # Check the historic list and flag any species names that should be considered part of the 2005 National List\n",
    "    df_src[\"historic_list\"] = df_src.apply(lambda x: check_historic_list(x[\"scientific name\"]), axis=1)\n",
    "\n",
    "    source_data.append(df_src)\n",
    "\n",
    "# Put the individual dataframes together into one overall set\n",
    "df_source = pd.concat(source_data, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source.to_csv(\"sgcn_source_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
